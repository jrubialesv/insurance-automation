{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Failed bounding boxes model\n",
    "WARNING: This is put here as example only, didn't use this approach at the end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#library imports\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import csv\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import albumentations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Looking at data\n",
    "path_to_csv = r'C:\\Users\\ismae\\Desktop\\3rd-year-uni\\Statistical-Learning\\final_project\\data\\parte_amistoso_6_35_3.csv'\n",
    "df_test = pd.read_csv(path_to_csv)\n",
    "df_test = df_test.drop(columns=['Unnamed: 0.1', 'Unnamed: 0'])\n",
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking data types\n",
    "print(df_test.dtypes)\n",
    "str(df_test['text'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to aggregate all data into one .csv file\n",
    "\n",
    "def combine_csv_jpg(folder_path):\n",
    "    csv_files = []\n",
    "    jpg_files = []\n",
    "\n",
    "    # Get all csv and jpg files in the folder\n",
    "    for file in os.listdir(folder_path):\n",
    "        if file.endswith(\".csv\"):\n",
    "            csv_files.append(file)\n",
    "        elif file.endswith(\".jpg\"):\n",
    "            jpg_files.append(file)\n",
    "\n",
    "    # Create a dictionary of csv and jpg file names\n",
    "    file_dict = {}\n",
    "    for csv_file in csv_files:\n",
    "        csv_file_name = csv_file.split(\".csv\")[0]\n",
    "        for jpg_file in jpg_files:\n",
    "            jpg_file_name = jpg_file.split(\".jpg\")[0]\n",
    "            if csv_file_name == jpg_file_name:\n",
    "                file_dict[csv_file] = jpg_file\n",
    "\n",
    "    # Add jpg file name column to csv files\n",
    "    for csv_file, jpg_file in file_dict.items():\n",
    "        df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "        df['jpg_file_name'] = jpg_file\n",
    "        df.to_csv(os.path.join(folder_path, csv_file), index=False)\n",
    "\n",
    "    # Aggregate all csv files into one file\n",
    "    all_files = []\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(os.path.join(folder_path, csv_file))\n",
    "        all_files.append(df)\n",
    "    combined_csv = pd.concat(all_files, ignore_index=True)\n",
    "    combined_csv.to_csv(os.path.join(folder_path, \"all_files.csv\"), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_csv_jpg(r\"C:\\Users\\ismae\\Desktop\\3rd-year-uni\\Statistical-Learning\\final_project\\data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_csv = r'C:\\Users\\ismae\\Desktop\\3rd-year-uni\\Statistical-Learning\\final_project\\data\\all_files.csv'\n",
    "df = pd.read_csv(path_to_csv, low_memory=False)\n",
    "df = df.drop(columns=['Unnamed: 0.1', 'Unnamed: 0'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking new column\n",
    "print('.unique() of column \"jpg_file_name: \"', df['jpg_file_name'].unique())\n",
    "print('\\nAmmount of pictures: ', len(df['jpg_file_name'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting new instances that are not valid in the format\n",
    "def remove_non_parte_amistoso_rows(df):\n",
    "    # Create a boolean mask of rows that contain the string \"parte_amistoso_\" in the 'jpg_file_name' column\n",
    "    mask = df['jpg_file_name'].str.contains('parte_amistoso_')\n",
    "\n",
    "    # Filter the DataFrame using the mask\n",
    "    filtered_df = df[mask]\n",
    "\n",
    "    # Return the filtered DataFrame\n",
    "    return filtered_df\n",
    "\n",
    "df = remove_non_parte_amistoso_rows(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_rows_with_texto_maquina(dataframe):\n",
    "    # create a boolean mask where campo column contains 'texto maquina'\n",
    "    mask = dataframe['campo'].str.contains('texto maquina')\n",
    "    \n",
    "    # filter out the rows that match the mask\n",
    "    dataframe = dataframe[~mask]\n",
    "    \n",
    "    # return the filtered dataframe\n",
    "    return dataframe\n",
    "\n",
    "# apply the function to delete rows containing 'texto maquina'\n",
    "df_filtered = delete_rows_with_texto_maquina(df)\n",
    "print('Count of texto maquinas: ', len(df_filtered[df_filtered['campo'] == 'texto maquina']))\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.to_csv(r'C:\\Users\\ismae\\Desktop\\3rd-year-uni\\Statistical-Learning\\final_project\\before_cleaned_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded = df_filtered\n",
    "# create a LabelEncoder object\n",
    "le = LabelEncoder()\n",
    "\n",
    "# fit and transform the 'campo' column\n",
    "df_encoded['campo'] = le.fit_transform(df_encoded['campo'])\n",
    "\n",
    "# print the encoding used for each label\n",
    "classes = list(le.classes_)\n",
    "encoding = list(le.transform(le.classes_))\n",
    "print(classes)\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_classes = pd.DataFrame({'classes': classes, 'encoding': encoding})\n",
    "df_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded['w'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_encoded.to_csv(r'C:\\Users\\ismae\\Desktop\\3rd-year-uni\\Statistical-Learning\\final_project\\cleaned_data.csv')\n",
    "df_classes.to_csv(r'C:\\Users\\ismae\\Desktop\\3rd-year-uni\\Statistical-Learning\\final_project\\classes.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adjusting dataset to Transformers format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df):\n",
    "    grouped = df.groupby('jpg_file_name')\n",
    "    dataset = []\n",
    "    for i, (filename, group) in enumerate(grouped):\n",
    "        print('Loading: ', filename)\n",
    "        image = Image.open('data/' + filename)\n",
    "        width, height = image.size\n",
    "        objects = {'id': [], 'area': [], 'bbox': [], 'category': []}\n",
    "        for j, (_, row) in enumerate(group.iterrows()):\n",
    "            object_info = {}\n",
    "            object_info['id'] = row['campo'] + j\n",
    "            object_info['area'] = int(row['w']) * int(row['h'])  # convert to int before multiplying\n",
    "            object_info['bbox'] = [int(row['x']), int(row['y']), int(row['w']) if int(row['w']) > 0 else 1, int(row['h']) if int(row['h']) > 0 else 1]\n",
    "            object_info['category'] = row['campo']\n",
    "            objects['id'].append(object_info['id'])\n",
    "            objects['area'].append(object_info['area'])\n",
    "            objects['bbox'].append(object_info['bbox'])\n",
    "            objects['category'].append(object_info['category'])\n",
    "        dataset.append({'image_id': i, 'image': image, 'width': width, 'height': height, 'objects': objects})\n",
    "    return dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_transformers = create_dataset(df_encoded)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### New format\n",
    "- image_id: the example image id\n",
    "- image: a PIL.Image.Image object containing the image\n",
    "- width: width of the image\n",
    "- height: height of the image\n",
    "- objects: a dictionary containing bounding box metadata for the objects in the image:\n",
    "    - id: the annotation id\n",
    "    - area: the area of the bounding box\n",
    "    - bbox: the objectâ€™s bounding box (in the COCO format )\n",
    "    - category: the objectâ€™s category, with possible all values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check dataset\n",
    "df_transformers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in df_transformers[0]['objects'].keys():\n",
    "    print('Length of', o,':', len(df_transformers[0]['objects'][o]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df_transformers[300:500]\n",
    "test_data = df_transformers[2220:]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change bbox to ints\n",
    "for entry in df_transformers:\n",
    "    entry['objects']['bbox'] = [[int(val) for val in bbox] for bbox in entry['objects']['bbox']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "\n",
    "# Create a Dataset object for each split\n",
    "train_dataset = Dataset.from_list(train_data)\n",
    "test_dataset = Dataset.from_list(test_data)\n",
    "\n",
    "my_dataset = DatasetDict({\n",
    "    'train': train_dataset,\n",
    "    'test': test_dataset,\n",
    "})\n",
    "\n",
    "print(my_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"facebook/detr-resnet-50\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = albumentations.Compose(\n",
    "    [\n",
    "        albumentations.Resize(640, 640),\n",
    "        albumentations.HorizontalFlip(p=1.0),\n",
    "        albumentations.RandomBrightnessContrast(p=1.0),\n",
    "    ],\n",
    "    bbox_params=albumentations.BboxParams(format=\"coco\", label_fields=[\"category\"]),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatted_anns(image_id, category, area, bbox):\n",
    "    annotations = []\n",
    "    for i in range(0, len(category)):\n",
    "        new_ann = {\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": category[i],\n",
    "            \"isCrowd\": 0,\n",
    "            \"area\": area[i],\n",
    "            \"bbox\": list(bbox[i]),\n",
    "        }\n",
    "        annotations.append(new_ann)\n",
    "\n",
    "    return annotations\n",
    "\n",
    "def transform_aug_ann(examples):\n",
    "    image_ids = examples[\"image_id\"]\n",
    "    images, bboxes, area, categories = [], [], [], []\n",
    "    for image, objects in zip(examples[\"image\"], examples[\"objects\"]):\n",
    "        image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\n",
    "        out = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"])\n",
    "\n",
    "        area.append(objects[\"area\"])\n",
    "        images.append(out[\"image\"])\n",
    "        bboxes.append(out[\"bboxes\"])\n",
    "        categories.append(out[\"category\"])\n",
    "\n",
    "    targets = [\n",
    "        {\"image_id\": id_, \"annotations\": formatted_anns(id_, cat_, ar_, box_)}\n",
    "        for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)\n",
    "    ]\n",
    "\n",
    "    return image_processor(images=images, annotations=targets, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_dataset['train'] = my_dataset['train'].with_transform(transform_aug_ann)\n",
    "my_dataset['train'][16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    pixel_values = [item[\"pixel_values\"] for item in batch]\n",
    "    encoding = image_processor.pad_and_create_pixel_mask(pixel_values, return_tensors=\"pt\")\n",
    "    labels = [item[\"labels\"] for item in batch]\n",
    "    batch = {}\n",
    "    batch[\"pixel_values\"] = encoding[\"pixel_values\"]\n",
    "    batch[\"pixel_mask\"] = encoding[\"pixel_mask\"]\n",
    "    batch[\"labels\"] = labels\n",
    "    return batch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training Dert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = list(le.classes_)\n",
    "\n",
    "id2label = {index: x for index, x in enumerate(categories, start=0)}\n",
    "label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForObjectDetection\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"detr-resnet-50_finetuned_OCR\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=10,\n",
    "    save_steps=200,\n",
    "    logging_steps=50,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=1e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=collate_fn,\n",
    "    train_dataset=my_dataset[\"train\"],\n",
    "    tokenizer=image_processor,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# format annotations the same as for training, no need for data augmentation\n",
    "def val_formatted_anns(image_id, objects):\n",
    "    annotations = []\n",
    "    for i in range(0, len(objects[\"id\"])):\n",
    "        new_ann = {\n",
    "            \"id\": objects[\"id\"][i],\n",
    "            \"category_id\": objects[\"category\"][i],\n",
    "            \"iscrowd\": 0,\n",
    "            \"image_id\": image_id,\n",
    "            \"area\": objects[\"area\"][i],\n",
    "            \"bbox\": objects[\"bbox\"][i],\n",
    "        }\n",
    "        annotations.append(new_ann)\n",
    "\n",
    "    return annotations\n",
    "\n",
    "\n",
    "# Save images and annotations into the files torchvision.datasets.CocoDetection expects\n",
    "def save_OCR_annotation_file_images(cppe5):\n",
    "    output_json = {}\n",
    "    path_output_OCR = f\"{os.getcwd()}/OCR/\"\n",
    "\n",
    "    if not os.path.exists(path_output_OCR):\n",
    "        os.makedirs(path_output_OCR)\n",
    "\n",
    "    path_anno = os.path.join(path_output_OCR, \"OCR_ann.json\")\n",
    "    categories_json = [{\"supercategory\": \"none\", \"id\": id, \"name\": id2label[id]} for id in id2label]\n",
    "    output_json[\"images\"] = []\n",
    "    output_json[\"annotations\"] = []\n",
    "    for example in cppe5:\n",
    "        ann = val_formatted_anns(example[\"image_id\"], example[\"objects\"])\n",
    "        output_json[\"images\"].append(\n",
    "            {\n",
    "                \"id\": example[\"image_id\"],\n",
    "                \"width\": example[\"image\"].width,\n",
    "                \"height\": example[\"image\"].height,\n",
    "                \"file_name\": f\"{example['image_id']}.png\",\n",
    "            }\n",
    "        )\n",
    "        output_json[\"annotations\"].extend(ann)\n",
    "    output_json[\"categories\"] = categories_json\n",
    "\n",
    "    with open(path_anno, \"w\") as file:\n",
    "        json.dump(output_json, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    for im, img_id in zip(cppe5[\"image\"], cppe5[\"image_id\"]):\n",
    "        path_img = os.path.join(path_output_OCR, f\"{img_id}.png\")\n",
    "        im.save(path_img)\n",
    "\n",
    "    return path_output_OCR, path_anno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "\n",
    "\n",
    "class CocoDetection(torchvision.datasets.CocoDetection):\n",
    "    def __init__(self, img_folder, feature_extractor, ann_file):\n",
    "        super().__init__(img_folder, ann_file)\n",
    "        self.feature_extractor = feature_extractor\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # read in PIL image and target in COCO format\n",
    "        img, target = super(CocoDetection, self).__getitem__(idx)\n",
    "\n",
    "        # preprocess image and target: converting target to DETR format,\n",
    "        # resizing + normalization of both image and target)\n",
    "        image_id = self.ids[idx]\n",
    "        target = {\"image_id\": image_id, \"annotations\": target}\n",
    "        encoding = self.feature_extractor(images=img, annotations=target, return_tensors=\"pt\")\n",
    "        pixel_values = encoding[\"pixel_values\"].squeeze()  # remove batch dimension\n",
    "        target = encoding[\"labels\"][0]  # remove batch dimension\n",
    "\n",
    "        return {\"pixel_values\": pixel_values, \"labels\": target}\n",
    "\n",
    "\n",
    "im_processor = AutoImageProcessor.from_pretrained(\"ismadoukkali/detr-resnet-50_finetuned_OCR\")\n",
    "\n",
    "path_output_OCR, path_anno = save_OCR_annotation_file_images(my_dataset[\"test\"])\n",
    "test_ds_coco_format = CocoDetection(path_output_OCR, im_processor, path_anno)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"ismadoukkali/detr-resnet-50_finetuned_OCR\")\n",
    "module = evaluate.load(\"ybelkada/cocoevaluate\", coco=test_ds_coco_format.coco)\n",
    "val_dataloader = torch.utils.data.DataLoader(\n",
    "    test_ds_coco_format, batch_size=8, shuffle=False, num_workers=0, collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx, batch in enumerate(tqdm(val_dataloader)):\n",
    "        pixel_values = batch[\"pixel_values\"]\n",
    "        pixel_mask = batch[\"pixel_mask\"]\n",
    "\n",
    "        labels = [\n",
    "            {k: v for k, v in t.items()} for t in batch[\"labels\"]\n",
    "        ]  # these are in DETR format, resized + normalized\n",
    "\n",
    "        # forward pass\n",
    "        outputs = model(pixel_values=pixel_values, pixel_mask=pixel_mask)\n",
    "\n",
    "        orig_target_sizes = torch.stack([target[\"orig_size\"] for target in labels], dim=0)\n",
    "        results = im_processor.post_process(outputs, orig_target_sizes)  # convert outputs of model to COCO api\n",
    "\n",
    "        module.add(prediction=results, reference=labels)\n",
    "        del batch\n",
    "\n",
    "results = module.compute()\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import requests\n",
    "\n",
    "filename = 'parte_amistoso_1_20_3.jpg'\n",
    "image = Image.open('data/' + filename)\n",
    "\n",
    "obj_detector = pipeline(model=\"ismadoukkali/detr-resnet-50_finetuned_OCR\")\n",
    "obj_detector(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"ismadoukkali/detr-resnet-50_finetuned_OCR\")\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"ismadoukkali/detr-resnet-50_finetuned_OCR\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    target_sizes = torch.tensor([image.size[::-1]])\n",
    "    results = image_processor.post_process_object_detection(outputs, threshold=0.5, target_sizes=target_sizes)[0]\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "        f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "draw = ImageDraw.Draw(image)\n",
    "print(results)\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    print(box)\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    x, y, x2, y2 = tuple(box)\n",
    "    draw.rectangle((x, y, x2, y2), outline=\"red\", width=1)\n",
    "    draw.text((x, y), model.config.id2label[label.item()], fill=\"white\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.patches as patches\n",
    "# Load the image\n",
    "img_path = r'C:\\Users\\ismae\\Desktop\\3rd-year-uni\\Statistical-Learning\\final_project\\data\\parte_amistoso_6_35_3.jpg'\n",
    "img = Image.open(img_path)\n",
    "\n",
    "# Create a figure and axes\n",
    "fig, ax = plt.subplots(figsize=(35, 35))\n",
    "\n",
    "# Display the image on the axes\n",
    "ax.imshow(img)\n",
    "\n",
    "# Loop through the DataFrame and plot the bounding boxes on the image\n",
    "for i, row in df_test.iterrows():\n",
    "    if row['campo'] == 'texto maquina':\n",
    "        continue\n",
    "    x, y, w, h = row['x'], row['y'], row['w'], row['h']\n",
    "    object_name = row['campo']\n",
    "    rect = patches.Rectangle((x, y), w, h, linewidth=2, edgecolor='r', facecolor='none')\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x, y-10, object_name, color='r', fontsize=12)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
